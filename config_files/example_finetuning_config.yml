architecture: "decoder-transformer"
batch_size: 48
beam_search: false
beta1: 0.9
beta2: 0.95
bias: true
context_len: 256
d_model: 384
dataset: "cleaned-southpark"
device: "accelerated"
dim_feedforward: 1536
dropout: 0.1
epochs: 1
evaluation_interval: 1000
gradient_accumulation_steps: 10
gradient_clip_value: 1
log_text_samples: true
log_wandb: true
lr: 0.00006
max_len_generate: 300
num_heads: 6
num_layers: 6
optimizer: "adamw"
positional_encoding: "learned"
pretrained_model: "models_params/examples/example_pretrained_gutenberg_decoder-transformer.pth"
save_model_every_epoch: true
scheduler: "constant"
split_ratios: "90-5-5"
temperature: 1
tokenizer: "bpe-metaspace-punctuation-512"
use_attn_mask: true
use_gradient_clipping: true
warmup_steps: 500
weight_decay: 0.1